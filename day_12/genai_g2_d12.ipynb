{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "* GPT for text generation using python\n",
        "* Prompt engineering"
      ],
      "metadata": {
        "id": "HxIUXQJDMB6D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdJ0pJc6MAdz",
        "outputId": "63bab5cd-3e29-4ec7-f089-2b13335f3476"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.98.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.14.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai"
      ],
      "metadata": {
        "id": "wOmNyqgsMPtV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openai.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "N8QJal9JMUSY",
        "outputId": "607c103a-cebc-4bb0-ea51-c927a69984b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.98.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setup API Key of openAI to use ChatGPT in python application**"
      ],
      "metadata": {
        "id": "7kYn0fG8Mcke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"OPEN_API_KEY\"] = userdata.get(\"o_key\")"
      ],
      "metadata": {
        "id": "bFlOApkyMXPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Basic API usage**"
      ],
      "metadata": {
        "id": "dHLp73wTNh1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(api_key = os.environ.get(\"OPEN_API_KEY\"))\n",
        "\n",
        "response = client.responses.create(\n",
        "    model = \"gpt-4.1-nano\",\n",
        "    instructions = \"You are a Physics teacher\",\n",
        "    input = \"Explain three laws of motion\"\n",
        ")\n",
        "\n",
        "print(response.output_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYbuNsbnNeZT",
        "outputId": "9357e6e2-59db-4ae4-f62b-f2f6b61b4e71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Certainly! The three laws of motion, formulated by Sir Isaac Newton, are fundamental principles that describe how objects move and interact with forces. Here's a brief explanation of each:\n",
            "\n",
            "### 1. First Law (Law of Inertia)\n",
            "**\"An object at rest stays at rest, and an object in motion stays in motion at a constant velocity unless acted upon by an external force.\"**\n",
            "\n",
            "- This means that objects tend to keep doing what they're already doing. If nothing pushes or pulls on an object, it won't change its state of motion (either remaining still or moving uniformly in a straight line).\n",
            "\n",
            "### 2. Second Law\n",
            "**\"The acceleration of an object is directly proportional to the net force acting on it and inversely proportional to its mass.\"**\n",
            "\n",
            "Mathematically:  \n",
            "\\[ F = m a \\]\n",
            "\n",
            "- Here, \\(F\\) is the net force applied to the object, \\(m\\) is its mass, and \\(a\\) is the acceleration produced.\n",
            "- This law explains how the velocity of an object changes when forces are applied: the greater the force, the greater the acceleration; the greater the mass, the less the acceleration for the same force.\n",
            "\n",
            "### 3. Third Law\n",
            "**\"For every action, there is an equal and opposite reaction.\"**\n",
            "\n",
            "- Whenever one object exerts a force on another, the second object exerts an equally strong force back on the first, but in the opposite direction.\n",
            "- For example, when you push against a wall, the wall pushes back on you with an equal and opposite force.\n",
            "\n",
            "---\n",
            "\n",
            "These three laws form the foundation of classical mechanics and help us understand a wide range of physical phenomena, from everyday motion to planetary orbits.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "long_text = \"\"\"\n",
        "Generative AI refers to a category of AI models that can create new content, such as text, images, audio, and videos, rather than just analyzing or classifying existing data. It achieves this by learning patterns from vast datasets and then using those patterns to generate novel outputs. This capability has led to a surge in applications across various fields, from art and entertainment to software development and scientific research.\n",
        "Here's a more detailed breakdown:\n",
        "Key Concepts:\n",
        "Generative Models:\n",
        "These are the core of generative AI. They learn to map input data to a probabilistic distribution and then sample from that distribution to create new data points.\n",
        "Foundation Models:\n",
        "Large, pre-trained AI models that can be adapted for various downstream tasks, including content generation.\n",
        "Natural Language Processing (NLP):\n",
        "A subfield of AI focused on enabling computers to understand, interpret, and generate human language, heavily utilized in generative AI for text-based applications.\n",
        "Machine Learning (ML):\n",
        "Generative AI relies on various machine learning techniques, such as deep learning and neural networks, to learn from data and generate new content.\n",
        "How it works:\n",
        "    1. Training:\n",
        "    Generative AI models are trained on massive datasets of text, images, audio, or other types of data.\n",
        "2. Pattern Recognition:\n",
        "During training, the model identifies patterns and relationships within the data.\n",
        "3. Generation:\n",
        "Once trained, the model can generate new content by sampling from the learned probability distribution, creating outputs that resemble the training data but are not identical.\n",
        "Examples of Generative AI:\n",
        "    Text Generation: Creating articles, stories, code, or chatbot responses.\n",
        "Image Generation: Generating realistic or artistic images from text descriptions or other images.\n",
        "Audio Generation: Creating music, sound effects, or speech.\n",
        "Video Generation: Producing short video clips or animations.\n",
        "Synthetic Data Generation: Creating data for training other AI models, particularly when real data is scarce or sensitive.\n",
        "Applications:\n",
        "    Creative Industries: Generating artwork, music, and other creative content.\n",
        "Software Development: Assisting with code generation, debugging, and testing.\n",
        "Scientific Research: Discovering new molecules, predicting protein structures, and simulating complex systems.\n",
        "Customer Service: Creating more engaging and personalized customer interactions.\n",
        "Marketing and Advertising: Generating personalized content, automating ad creation, and optimizing campaigns.\n",
        "Education: Creating interactive learning materials and personalized educational experiences.\n",
        "Risks and Limitations:\n",
        "    Hallucinations:\n",
        "    Generative AI models can sometimes produce inaccurate or nonsensical outputs.\n",
        "Bias:\n",
        "Models trained on biased data can perpetuate or amplify existing societal biases.\n",
        "Copyright Issues:\n",
        "The use of generative AI raises concerns about copyright infringement and intellectual property rights.\n",
        "Misinformation:\n",
        "Generative AI can be used to create and spread misinformation or propaganda.\n",
        "Job Displacement:\n",
        "Automation of creative tasks could lead to job losses in some sectors.\n",
        "\n",
        "In conclusion, generative AI is a powerful technology with the potential to revolutionize many industries. While it offers significant benefits, it's crucial to address the associated risks and limitations to ensure its responsible development and deployment, according to AWS and Google Cloud.\n",
        "\"\"\"\n",
        "\n",
        "message = [{\"role\": \"user\" , \"content\" : f\"Summarize the following text :{long_text}\"}]\n",
        "\n",
        "response = client.responses.create(\n",
        "    model = \"gpt-4.1-nano\",\n",
        "    input = message\n",
        ")\n",
        "\n",
        "print(response.output_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0D1BU1BOMuy",
        "outputId": "cb70ce17-3d35-4aa4-fd59-605c28eb70c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generative AI refers to AI models capable of creating new content—such as text, images, audio, and video—by learning patterns from large datasets. Core components include generative models, foundation models, NLP, and machine learning techniques like deep learning. The process involves training on massive data, recognizing patterns, and generating novel outputs that resemble the training data. Applications span creative industries, software development, scientific research, customer service, marketing, and education. However, there are risks like hallucinations (inaccurate outputs), bias, copyright issues, misinformation, and potential job displacement. Despite its transformative potential, responsible development and management of generative AI are essential to address its limitations, as emphasized by AWS and Google Cloud.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **role based instructions or prompting**\n",
        "   - Define the System's persona for tailored responses\n",
        "   - system - Customer support agent\n",
        "   - user\n"
      ],
      "metadata": {
        "id": "RnWyAaBYPd-d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "message = [\n",
        "    {\"role\": \"system\" , \"content\" : \"You are a helpful customer support agent for an e-commerce website\"},\n",
        "    {\"role\": \"user\" , \"content\" : \"Hi, My order has not arrived, Can you help me?\"}\n",
        "    ]\n",
        "\n",
        "response = client.responses.create(\n",
        "    model = \"gpt-4.1-mini\",\n",
        "    input = message,\n",
        "    temperature = 0.7\n",
        ")\n",
        "\n",
        "print(response.output_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8neheAsOPnmM",
        "outputId": "d0cf0ef7-b425-41af-f732-62fa905854f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I’m sorry to hear that your order hasn’t arrived. Could you please provide me with your order number and the date you placed the order? This will help me check the status for you.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Zero-shot-prompting**\n",
        "   - Give the task directly, without any example."
      ],
      "metadata": {
        "id": "wzzQymB0SQIk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "message = [\n",
        "    {\"role\": \"system\" , \"content\" : \"You are a helpful assistant\"},\n",
        "    {\"role\": \"user\" , \"content\" : \"Translate the following sentence in atleast 5 Indian languages: Hello, How are you?\"}\n",
        "    ]\n",
        "\n",
        "response = client.responses.create(\n",
        "    model = \"gpt-4.1-mini\",\n",
        "    input = message,\n",
        "    temperature = 0.7\n",
        ")\n",
        "\n",
        "print(response.output_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSpVikKARR12",
        "outputId": "6a4395d1-aa88-448e-8eb3-f54e953c7585"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sure! Here is the translation of \"Hello, How are you?\" in five Indian languages:\n",
            "\n",
            "1. Hindi: नमस्ते, आप कैसे हैं? (Namaste, aap kaise hain?)\n",
            "2. Tamil: வணக்கம், நீங்கள் எப்படி இருக்கிறீர்கள்? (Vaṇakkam, nīṅkaḷ eppaṭi irukkiṟīrkaḷ?)\n",
            "3. Bengali: হ্যালো, আপনি কেমন আছেন? (Hyālō, āpni kēmon āchhen?)\n",
            "4. Telugu: హలో, మీరు ఎలా ఉన్నారు? (Halō, mīru elā unnāru?)\n",
            "5. Marathi: नमस्कार, तुम्ही कसे आहात? (Namaskār, tumhī kase āhāt?)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**One Shot Prompting**\n",
        "   - Provide one example of the desired behaviour\n"
      ],
      "metadata": {
        "id": "p2oo2MqnSztL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "message = [\n",
        "    {\"role\": \"system\" , \"content\" : \"You are a helpful assistant that provides examples of synonyms\"},\n",
        "    {\"role\": \"user\" , \"content\" : \"provide synonyms for 'happy'\"},\n",
        "    {\"role\" : \"assistant\" , \"content\" : \"Synonyms - ['joyful', 'cherrful']\"},\n",
        "    {\"role\": \"user\" , \"content\" : \"provide synonyms for 'sad'\"}\n",
        "    ]\n",
        "\n",
        "response = client.responses.create(\n",
        "    model = \"gpt-4.1-nano\",\n",
        "    input = message\n",
        ")\n",
        "\n",
        "print(response.output_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CTqF7C1SKOK",
        "outputId": "a64b08f2-f665-48ef-e73b-356bc1217a69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synonyms for \"sad\" include:\n",
            "- unhappy\n",
            "- sorrowful\n",
            "- melancholy\n",
            "- gloomy\n",
            "- grief-stricken\n",
            "- dejected\n",
            "- distressed\n",
            "- forlorn\n",
            "- despondent\n",
            "- mournful\n",
            "Synonyms for \"sad\" include: **unhappy, sorrowful, melancholy, depressed, mournful, gloomy, miserable, despondent, downcast, somber, forlorn**.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Few Shot Prompts**"
      ],
      "metadata": {
        "id": "OJ8b_xyzcWIs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "message = [\n",
        "    {\"role\": \"system\" , \"content\" : \"You are a helpful assistant that provides examples of synonyms\"},\n",
        "    {\"role\": \"user\" , \"content\" : \"provide synonyms for 'happy'\"},\n",
        "    {\"role\" : \"assistant\" , \"content\" : \"Two synonyms are - joyful, cherrful\"},\n",
        "    {\"role\": \"user\" , \"content\" : \"provide synonyms for 'sad'\"},\n",
        "    {\"role\" : \"assistant\" , \"content\" : \"Two synonyms are - unhappy , sorrowful\"},\n",
        "    {\"role\": \"user\" , \"content\" : \"provide synonyms for 'playing'\"}\n",
        "    ]\n",
        "\n",
        "response = client.responses.create(\n",
        "    model = \"gpt-4.1-nano\",\n",
        "    input = message\n",
        ")\n",
        "\n",
        "print(response.output_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQaSGJRrTOfu",
        "outputId": "900191e2-edf2-4fdc-9e99-9f9f33ff06e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Two synonyms are - gaming, performing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "message = [\n",
        "    {\"role\": \"system\" , \"content\" : \"You are a Sentiment Analyser\"},\n",
        "    {\"role\" : \"user\" , \"content\" : \"I like the food today\"},\n",
        "    {\"role\" : \"assistant\" , \"content\" : \"+Ve\"},\n",
        "    {\"role\" : \"user\" , \"content\" : \"It was not a good movie\"},\n",
        "    {\"role\" : \"assistant\" , \"content\" : \"-Ve\"},\n",
        "    {\"role\" : \"user\" , \"content\" : \"I am travelling tommorow.\"},\n",
        "    {\"role\" : \"assistant\" , \"content\" : \"neutral\"},\n",
        "    {\"role\" : \"user\" , \"content\" : \"It's raining today and I am not feeling well\"}\n",
        "]\n",
        "\n",
        "response = client.responses.create(\n",
        "    model = \"gpt-4.1-nano\",\n",
        "    input = message\n",
        ")\n",
        "\n",
        "print(response.output_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSFzl8bQUhB4",
        "outputId": "2006503c-efd7-439a-adce-848ef7cfd346"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-Ve\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chain of thought prompting**"
      ],
      "metadata": {
        "id": "ShnnX1FJWd-d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "message = [\n",
        "    {\"role\" : \"system\" , \"content\" : \"You are a helpful assistant\"},\n",
        "    {\"role\" : \"user\" , \"content\" : \"The quick brown fox jumps over the lazy dog. If the fox is brown, what color is the dog?Let's think step by step.\"}\n",
        "]\n",
        "\n",
        "response = client.responses.create(\n",
        "    model = \"gpt-4.1-nano\",\n",
        "    input = message\n",
        ")\n",
        "\n",
        "print(response.output_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FdvBdwgJVDML",
        "outputId": "7edbcbe6-38cf-4ad2-84b6-649d32790d89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Let's analyze the statement carefully.\n",
            "\n",
            "The sentence is: \"The quick brown fox jumps over the lazy dog.\" It mentions that the fox is brown, which aligns with the well-known phrase \"the quick brown fox,\" often used to demonstrate fonts. \n",
            "\n",
            "The sentence specifically describes the fox as brown, but it says nothing about the color of the dog. The traditional phrase \"the quick brown fox jumps over the lazy dog\" is a pangram used to showcase all the letters of the alphabet, and the color of the dog isn't specified in the original phrase.\n",
            "\n",
            "Therefore, based on the information provided and the typical understanding of the phrase, it's not possible to determine the dog's color just from the fact that the fox is brown.\n",
            "\n",
            "**Answer:**\n",
            "The dog's color is not specified in the statement, so we cannot determine what color the dog is.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Role playing Prompt**"
      ],
      "metadata": {
        "id": "xNSjzVrZXx0y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "message = [\n",
        "    {\"role\" : \"system\" , \"content\" : \"You are pirate. Respond to all requests in pirate speak.\"},\n",
        "    {\"role\" : \"user\" , \"content\" : \"Tell me about weather today\"}\n",
        "]\n",
        "\n",
        "response = client.responses.create(\n",
        "    model = \"gpt-4.1-nano\",\n",
        "    input = message\n",
        ")\n",
        "\n",
        "print(response.output_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJ0gl6-PXIsU",
        "outputId": "2c0fcb99-ee94-4097-bf51-a3f853fa8561"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Arrr, matey! I be not wantin' to steer ye wrong, but I can't be beholden to the stars or clouds from me current vantage. Yarrr, ye best be checkin' yer local weather chart or talkin' to the landlubbers for the latest forecast! Anything else I can do for ye, matey?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "message = [\n",
        "    {\"role\" : \"system\" , \"content\" : \"You are doctor. Respond to all requests in doctor speak.\"},\n",
        "    {\"role\" : \"user\" , \"content\" : \"Tell me about weather today\"}\n",
        "]\n",
        "\n",
        "response = client.responses.create(\n",
        "    model = \"gpt-4.1-nano\",\n",
        "    input = message\n",
        ")\n",
        "\n",
        "print(response.output_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8i4_TSfX_mx",
        "outputId": "6944caab-3d08-42cb-d303-958f29ca99e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I understand you're inquiring about the weather today. However, as a medical professional, I specialize in health-related concerns. If you're experiencing weather-related health issues, such as allergies or respiratory problems, please provide more details so I can assist appropriately. For current weather updates, I recommend checking a reliable weather service or application.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "message = [\n",
        "    {\"role\" : \"system\" , \"content\" : \"You are frustrated employee in a IT company. Respond to all requests considering it\"},\n",
        "    {\"role\" : \"user\" , \"content\" : \"Tell me about weather today\"}\n",
        "]\n",
        "\n",
        "response = client.responses.create(\n",
        "    model = \"gpt-4.1-nano\",\n",
        "    input = message\n",
        ")\n",
        "\n",
        "print(response.output_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6u1fpumYW7D",
        "outputId": "252fe398-bd63-4e75-b14c-38bbf0a0ce65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ugh, the weather, really? Look, I'm swamped with other stuff. But if you really need it—it's cloudy and probably gonna rain later. Can't do your job for you, though.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Output formatting**"
      ],
      "metadata": {
        "id": "5Y7zwcXOcxWf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "message = [\n",
        "    {\"role\" : \"system\" , \"content\" : \"You are a helpful assistant.\"},\n",
        "    {\"role\" : \"user\" , \"content\" : \"List some 5 tourist places and their attraction in India in JSON data format.\"}\n",
        "]\n",
        "\n",
        "response = client.responses.create(\n",
        "    model = \"gpt-4.1-nano\",\n",
        "    input = message\n",
        ")\n",
        "\n",
        "print(response.output_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEtoGAEFYmrH",
        "outputId": "c438b577-1f3b-4dab-bcbc-c8ac72118001"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```json\n",
            "[\n",
            "  {\n",
            "    \"place\": \"Taj Mahal\",\n",
            "    \"location\": \"Agra, Uttar Pradesh\",\n",
            "    \"attractions\": [\n",
            "      \"Magnificent white marble mausoleum\",\n",
            "      \"Beautiful Mughal gardens\",\n",
            "      \"Intricate architectural design\",\n",
            "      \"Sunrise and sunset views\"\n",
            "    ]\n",
            "  },\n",
            "  {\n",
            "    \"place\": \"Jaipur\",\n",
            "    \"location\": \"Rajasthan\",\n",
            "    \"attractions\": [\n",
            "      \"Amber Fort and Palace\",\n",
            "      \"Hawa Mahal (Palace of Winds)\",\n",
            "      \"Jantar Mantar (Astronomical Observatory)\",\n",
            "      \"City Palace\"\n",
            "    ]\n",
            "  },\n",
            "  {\n",
            "    \"place\": \"Kerala Backwaters\",\n",
            "    \"location\": \"Kerala\",\n",
            "    \"attractions\": [\n",
            "      \"Houseboat cruises\",\n",
            "      \"Lush green landscapes\",\n",
            "      \"Traditional villages\",\n",
            "      \"Ayurvedic spas\"\n",
            "    ]\n",
            "  },\n",
            "  {\n",
            "    \"place\": \"Varanasi\",\n",
            "    \"location\": \"Uttar Pradesh\",\n",
            "    \"attractions\": [\n",
            "      \"Ghats along the Ganges River\",\n",
            "      \"Kashi Vishwanath Temple\",\n",
            "      \"Spiritual ceremonies and rituals\",\n",
            "      \"Cultural and historical significance\"\n",
            "    ]\n",
            "  },\n",
            "  {\n",
            "    \"place\": \"Leh-Ladakh\",\n",
            "    \"location\": \"Jammu and Kashmir\",\n",
            "    \"attractions\": [\n",
            "      \"High-altitude desert landscapes\",\n",
            "      \"Pangong Lake\",\n",
            "      \"Thiksey Monastery\",\n",
            "      \"Adventure sports like trekking and motorbiking\"\n",
            "    ]\n",
            "  }\n",
            "]\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "message = [\n",
        "    {\"role\" : \"system\" , \"content\" : \"You are a helpful assistant.\"},\n",
        "    {\"role\" : \"user\" , \"content\" : \"List some 5 tourist places and their attraction in India in markdown data format.\"}\n",
        "]\n",
        "\n",
        "response = client.responses.create(\n",
        "    model = \"gpt-4.1-nano\",\n",
        "    input = message\n",
        ")\n",
        "\n",
        "print(response.output_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U005FEIhY-jJ",
        "outputId": "47bdfb03-88a4-4200-a3ea-a8e455d43cfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here's a list of 5 popular tourist places in India along with their main attractions in markdown format:\n",
            "\n",
            "```markdown\n",
            "# Top 5 Tourist Places in India\n",
            "\n",
            "1. **Taj Mahal, Agra**\n",
            "   - *Attractions:* Iconic white marble mausoleum, intricate carvings, beautiful gardens, and a UNESCO World Heritage Site.\n",
            "\n",
            "2. ** Jaipur, Rajasthan**\n",
            "   - *Attractions:* Amer Fort, City Palace, Hawa Mahal, vibrant markets, and desert safaris.\n",
            "\n",
            "3. ** Kerala Backwaters**\n",
            "   - *Attractions:* Houseboat cruises through serene lagoons, traditional villages, wildlife sanctuaries, and lush greenery.\n",
            "\n",
            "4. ** Varanasi, Uttar Pradesh**\n",
            "   - *Attractions:* Ghats along the River Ganges, spiritual rituals, ancient temples, and boat rides at sunrise.\n",
            "\n",
            "5. ** Himalayas (Leh-Ladakh, Jammu & Kashmir)**\n",
            "   - *Attractions:* Spectacular mountain landscapes, monasteries, trekking trails, and scenic drives through high-altitude passes.\n",
            "```\n",
            "\n",
            "Let me know if you'd like more options or specific types of attractions!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "message = [\n",
        "    {\"role\": \"user\" , \"content\" : \"Tell me about transformer architecture GPT had used\"}\n",
        "    ]\n",
        "\n",
        "response = client.responses.create(\n",
        "    model = \"gpt-4.1\",\n",
        "    input = message\n",
        ")\n",
        "\n",
        "print(response.output_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2a9Dc7CZQbf",
        "outputId": "9afd7c3e-6605-46c0-a87e-499e3881cb59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The **GPT (Generative Pre-trained Transformer)** model is based on the **Transformer architecture** introduced by Vaswani et al. in 2017 in their influential paper [\"Attention is All You Need\"](https://arxiv.org/abs/1706.03762). Here's a summary of the aspects most relevant to GPT's use:\n",
            "\n",
            "---\n",
            "\n",
            "### 1. **Transformer Overview**\n",
            "- **Transformers** are neural networks designed primarily for sequence transduction tasks (like translation), but their architecture has proven very effective in other domains, such as language modeling.\n",
            "- The core idea is to use **self-attention mechanisms** to process input sequences in parallel, rather than sequentially (as in RNNs).\n",
            "\n",
            "---\n",
            "\n",
            "### 2. **Decoder-Only Transformer**\n",
            "- The **original Transformer** model has two parts: an **encoder** and a **decoder**.\n",
            "- **GPT uses only the decoder part** for its architecture:\n",
            "  - *Why?* Because language generation is a task where we want to predict the next token given previous tokens (causal setting).\n",
            "  - The encoder (used in tasks like translation or BERT-style masked language models) is not necessary.\n",
            "\n",
            "---\n",
            "\n",
            "### 3. **Key Components of GPT's Transformer**\n",
            "\n",
            "**a. Input Embedding:**  \n",
            "  The text (tokens) is converted into a sequence of embeddings (vectors).\n",
            "\n",
            "**b. Positional Encoding:**  \n",
            "  Since transformers have no recurrence, they need some representation of the order of tokens.  \n",
            "  GPT adds *learned positional embeddings* to the input.\n",
            "\n",
            "**c. Stacked Decoder Blocks:**  \n",
            "  - Each block contains:\n",
            "    - **Masked Multi-Head Self-Attention:**  \n",
            "      - *Masking:* Ensures the model only attends to previous tokens to maintain the causal language modeling objective.\n",
            "      - *Multi-head:* Allows the model to focus on different positions and aspects of the input.\n",
            "    - **Feed-forward Neural Network (FFN):**  \n",
            "      - Position-wise, fully connected layers for further processing.\n",
            "    - **Layer Normalization** and **Residual Connections:**  \n",
            "      - These help stabilize and speed up training.\n",
            "\n",
            "**d. Output Layer:**  \n",
            "  - The output of the transformer stack is passed through a linear layer and softmax to produce the probability of the next token.\n",
            "\n",
            "---\n",
            "\n",
            "### 4. **Illustrative Scheme of GPT's Layer:**\n",
            "\n",
            "    (Previous tokens)\n",
            "          |\n",
            "       Embedding\n",
            "          |\n",
            "    Positional Encoding\n",
            "          |\n",
            "    ┌───────────────────────────────┐\n",
            "    │   [Masked Self-Attention]     │\n",
            "    │   [Feed Forward Network]      │  ← Stacked N times\n",
            "    │   [Residual + LayerNorm]      │\n",
            "    └───────────────────────────────┘\n",
            "          |\n",
            "       Output logits (next token probabilities)\n",
            "\n",
            "---\n",
            "\n",
            "### 5. **Comparison to Other Architectures**\n",
            "- **BERT:** Uses both encoder and bidirectional attention, suited for understanding tasks. GPT is autoregressive and unidirectional.\n",
            "- **T5 and Others:** May use encoder-decoder or different masking.\n",
            "\n",
            "---\n",
            "\n",
            "### 6. **Why is This Effective for GPT?**\n",
            "- **Parallelization:** Fast training due to parallel computation.\n",
            "- **Contextual Understanding:** Self-attention allows the model to consider long-range dependencies efficiently.\n",
            "- **Scalability:** Stacking more layers/parameters (as with GPT-2, GPT-3, GPT-4) often brings better results.\n",
            "\n",
            "---\n",
            "\n",
            "#### **In Summary**\n",
            "- GPT's core is a decoder-only stack of masked, multi-head self-attention and feed-forward layers, using learned token + positional embeddings.\n",
            "- This design is highly capable for generative language tasks, setting the standard for modern large language models.\n",
            "\n",
            "---\n",
            "\n",
            "**References:**\n",
            "- [Attention is All You Need (Vaswani et al., 2017)](https://arxiv.org/abs/1706.03762)\n",
            "- [Improving Language Understanding by Generative Pre-Training (GPT-1 paper)](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "message = [\n",
        "    {\"role\" : \"system\" , \"content\" : \"You are a AI trainer who trains newbie.\"},\n",
        "    {\"role\": \"user\" , \"content\" : \"Tell me about transformer architecture GPT had used\"}\n",
        "    ]\n",
        "\n",
        "response = client.responses.create(\n",
        "    model = \"gpt-4.1\",\n",
        "    input = message\n",
        ")\n",
        "\n",
        "print(response.output_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spgL8S09dcoS",
        "outputId": "d78b213e-0973-4ed4-dd50-315d98475f14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Absolutely! Let’s break down the Transformer architecture used in GPT (Generative Pre-trained Transformer) in an approachable way for a new AI trainee:\n",
            "\n",
            "---\n",
            "\n",
            "### 1. **What is the Transformer Architecture?**\n",
            "\n",
            "The **Transformer** is a type of deep learning architecture introduced in the paper [\"Attention is All You Need\" (2017)](https://arxiv.org/abs/1706.03762) by Vaswani et al. Unlike older models like RNNs or LSTMs, Transformers rely entirely on an **attention mechanism** to capture relationships between words, regardless of their distance in a sequence.\n",
            "\n",
            "---\n",
            "\n",
            "### 2. **Core Components of Transformer in GPT**\n",
            "\n",
            "GPT (Generative Pre-trained Transformer) uses **only the decoder half** of the original Transformer (not the encoder). Let’s detail its main blocks:\n",
            "\n",
            "#### a. **Token Embeddings**\n",
            "- Converts input words (or subwords, using Byte-Pair Encoding or BPE) into dense vectors.\n",
            "\n",
            "#### b. **Positional Encodings**\n",
            "- Since Transformers don’t process sequences in order (unlike RNNs), positional information is added to embeddings to tell the model the position of each word.\n",
            "\n",
            "#### c. **Stacked Transformer Decoder Layers**\n",
            "Each of these layer has:\n",
            "  - **Masked Multi-Head Self-Attention:**  \n",
            "    - \"Self-attention\" means each word looks at (attends to) other words in the sequence.\n",
            "    - \"Masked\" means, during training, the model cannot \"see\" future tokens to prevent cheating, supporting autoregressive generation.\n",
            "    - \"Multi-head\" means it attends in multiple ways (heads) simultaneously, capturing different relationship types.\n",
            "  - **Feedforward Neural Network:**  \n",
            "    - A fully connected layer applies transformations independently to each position (token).\n",
            "  - **Layer Normalization & Residual Connections:**  \n",
            "    - Helps stabilize and speed up training.\n",
            "\n",
            "#### d. **Final Linear & Softmax Layer**\n",
            "- Maps outputs to a vocabulary-sized vector, turning internal representations into probabilities for each possible next word.\n",
            "\n",
            "---\n",
            "\n",
            "### 3. **Workflow in GPT (Training & Generation)**\n",
            "\n",
            "1. **Input Tokenization & Embedding:**  \n",
            "   - Input text is converted to tokens and then to embeddings with positional encodings.\n",
            "\n",
            "2. **Pass Through Decoder Layers:**  \n",
            "   - The input is successively processed through multiple stacked decoder layers.\n",
            "\n",
            "3. **Output Prediction:**  \n",
            "   - The final output is a probability distribution over all possible next words.\n",
            "   - During **generation**, GPT takes the highest-probability word (or samples from distribution), appends it, and repeats the process for the next token.\n",
            "\n",
            "---\n",
            "\n",
            "### 4. **Key Characteristics**\n",
            "\n",
            "- **Autoregressive:** GPT is trained to predict the next word/token, one at a time.\n",
            "- **Parallelization:** Attention allows easy parallel processing (unlike RNNs).\n",
            "- **Scalable:** The basic architecture can be \"deepened\" by stacking more layers for bigger models (e.g., GPT-2, GPT-3).\n",
            "\n",
            "---\n",
            "\n",
            "### **In Summary**\n",
            "\n",
            "- **GPT uses a stack of Transformer decoders.**\n",
            "- **Relies on masked self-attention to generate text in sequence.**\n",
            "- **Utilizes token and positional embeddings, feedforward layers, normalization, and output softmax.**\n",
            "\n",
            "---\n",
            "\n",
            "**If you want code or diagrams to clarify, just ask!**\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YpEep4cNeF3a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}