# -*- coding: utf-8 -*-
"""rag_gradio_g3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xft44SpRolDwDjqRgTnHCM_gTwkSv6II
"""

!pip install PyPDF2

!pip install langchain_community

!pip install langchain_google_genai

!pip install faiss-cpu

import os
import PyPDF2
from openai import OpenAI
import google.generativeai as genai
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.chat_models import ChatOpenAI
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.chains import RetrievalQA

import gradio as gr

os.environ['OPENAI_API_KEY'] = ''
os.environ['GOOGLE_API_KEY'] = ''

qa_chain = None

PDF_PATH = 'D:/RAG_UI/company_manual.pdf'

def extract_data_from_pdf(pdf_path):
    with open(pdf_path , 'rb') as file:
        pdfreader = PyPDF2.PdfReader(file)
        full_text = ''
        for page in pdfreader.pages:
            full_text += page.extract_text()
    return full_text

def split_text(text):
  splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=100)
  docs = splitter.create_documents([text])
  return docs

def create_vector_store(docs):
  embeddings = OpenAIEmbeddings()
  vectorstore = FAISS.from_documents(docs , embeddings)
  return vectorstore

def setup_rag_qa(vectorstore):
  retriever = vectorstore.as_retriever(search_type = 'similarity')
  #llm = ChatOpenAI(model = "gpt-4.1-nano")
  llm = ChatGoogleGenerativeAI(model = "gemini-2.5-flash")
  rag_chain = RetrievalQA.from_chain_type(llm=llm , retriever=retriever)
  return rag_chain

#Handle PDF upload and process
def upload_pdf(file):
  global qa_chain
  if file is None:
    return "No file Uploaded"

  try:
    text = extract_data_from_pdf(file.name)
    docs = split_text(text)
    vectorstore = create_vector_store(docs)
    qa_chain = setup_rag_qa(vectorstore)
    return "PDF uploaded and processed successfully"
  except Exception as err:
    return f"Error : {str(err)}"

#Handle question input and answers
def ask_question(query):
  result = qa_chain(query)
  return result['result']

with gr.Blocks() as ui_demo:
  gr.Markdown("# RAG assistant with GPT")
  gr.Markdown("Upload a PDF, Then ask any questions from its content.")

  with gr.Row():
    pdf_input = gr.File(label = "Upload PDF")
    upload_status = gr.Textbox(label = "Upload Status")

  pdf_input.change(fn = upload_pdf , inputs = pdf_input , outputs=upload_status)

  with gr.Row():
    question_input = gr.Textbox(label="Ask few questions")
    answer_output = gr.Textbox(label="Answer")

  question_input.submit(fn=ask_question , inputs = question_input , outputs = answer_output)

ui_demo.launch()

