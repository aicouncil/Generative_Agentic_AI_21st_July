# -*- coding: utf-8 -*-
"""genai_g2_d11.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dCDPGqv7tG_FJs7l_Wj2rEEjWxKcBBb1

**Question answering using Semantic(meaning) Search Workflow**

* 1. Install required dependecies
    - setence transformers - for generating semantic embeddings of sentences or texts using pre-trained models
    - PyPDF2 - For extracting text from PDF files
"""

!pip install sentence-transformers

!pip install PyPDF2

"""**2. Extract text data from a PDF file using PyPDF2**"""

import PyPDF2

def extract_data_from_pdf(pdf_path):
    with open(pdf_path , 'rb') as file:
        pdfreader = PyPDF2.PdfReader(file)
        full_text = ''
        for page in pdfreader.pages:
            full_text += page.extract_text()
    return full_text

extracted_text = extract_data_from_pdf('/content/company_manual.pdf')
print(extracted_text)

"""**3. Clean and preprocessing the extracted text**"""

import re

def clean_text(text):
    # remove extra spaces
    text = re.sub(r'\s+' , ' ' , text)
    # remove non-ascii characters (if any)
    text = re.sub(r'[^\x00-\x7F]+', '', text)
    return text

cleaned_text = clean_text(extracted_text)
print(cleaned_text)

"""**4. Load and use pre-trained Sentence Transformer model**
   - sentence-transformers library provides pre-trained models for generating semantic embeddings.
   - https://huggingface.co/sentence-transformers
   - Load a lightweight and powerful model capabale of producing 384-dimensional embeddings for sentences or paragraph.
"""

from sentence_transformers import SentenceTransformer
model = SentenceTransformer("all-MiniLM-L6-v2")

"""**5. Segmenting the document content into logical sections**"""

sections = {
    "About the Company": cleaned_text.split('About the Company')[1].split('Return Policy')[0],
    "Return Policy": cleaned_text.split('Return Policy')[1].split('Warranty')[0],
    "Warranty": cleaned_text.split('Warranty')[1].split('Customer Service')[0],
    "Customer Service": cleaned_text.split('Customer Service')[1].split('Environmental Commitment')[0],
    "Environmental Commitment": cleaned_text.split('Environmental Commitment')[1]
}

"""**6. Create Embeddings for each section and for a given query**
   - we encode both document sections and user's query into vector representations (embeddings).
"""

#Encode a query-
query = "How do I send the product back?"
#query = "My friend want to explore the company"
query_embedding = model.encode([query])[0]

#Encoding of each section
section_embeddings = {}

for title,content in sections.items():
  #print(title ,"-", content)
  section_embeddings[title] = model.encode([content])[0]

print(section_embeddings)

"""**7. Calculate the cosine similarity to identify the most relevant sections**
   - Cosine similarity score quantifies how similar two vectors are.
   - The section with the highest similarity to the query is the best match.
"""

import numpy as np

def cosine_similarity(a,b):
  return np.dot(a,b)/(np.linalg.norm(a) * np.linalg.norm(b))

#Calculate cosine similarity between query_embedding vector and vector of each section in your document

similarities = {}

for title,emb in section_embeddings.items():
  similarity_score = cosine_similarity(query_embedding , emb)
  similarities[title] = similarity_score

print(similarities)

"""**8.Visualising the similarity score**"""

import matplotlib.pyplot as plt            #python library for data plotting or visualisation

titles = list(similarities.keys())
scores = list(similarities.values())

print(titles)
print(scores)

plt.barh(titles , scores)               #horizontal bar plot
plt.show()

"""**Implementing a semantic search chatbot function**
   - A chatbot function that can take user query and returns the most relevant section based on sementic similarity.
"""

print(sections['About the Company'])

def semantic_search(query , sections , model):
  query_embedding = model.encode([query])[0]
  default_similarity = 0.0
  best_match = None

  for title,content in sections.items():
    section_embedding = model.encode([content])[0]
    similarity_score = cosine_similarity(query_embedding , section_embedding)
    if similarity_score > default_similarity:
      default_similarity = similarity_score
      best_match = title
  if best_match:
    return f"Bot: The most relevant section is {best_match}\nHere is the information:\n{sections[best_match]}"
  else:
    return "Bot: I couldn't find the relevant answer."

sections = {
    "About the Company": cleaned_text.split('About the Company')[1].split('Return Policy')[0],
    "Return Policy": cleaned_text.split('Return Policy')[1].split('Warranty')[0],
    "Warranty": cleaned_text.split('Warranty')[1].split('Customer Service')[0],
    "Customer Service": cleaned_text.split('Customer Service')[1].split('Environmental Commitment')[0],
    "Environmental Commitment": cleaned_text.split('Environmental Commitment')[1]
}

model = SentenceTransformer("all-MiniLM-L6-v2")

response = semantic_search("send the product back" , sections , model)
print(response)

